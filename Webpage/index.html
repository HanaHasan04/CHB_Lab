<!DOCTYPE html>
<html>
<head>
  <title>Sign Language Project</title>
  <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>
  <h1>Comparing Emotional Expression Across Hearing, Deaf, and CODA Individuals</h1>
  <p1>Hana Hasan & Michael Rodel <br> 
    <b>Superivsors:</b> Prof. Hagit Hel-Or, Rose Stamp, Svetlana Dachkovsky <br><br> </p1>

  <p3>Introduction</p3>
  <p>Effective emotional expression is a fundamental component of human communication, and it exhibits intriguing variations among individuals with different hearing abilities. Cultural, cognitive, physiological factors, as well as individual differences like hearing abilities, contribute to these variations. Exploring the differences in emotional expression across individuals with diverse hearing abilities can offer valuable insights into the intricate dynamics between communication and hearing.</p>

  <p3>Motivation</p3>
  <p> In this study, we aim to compare emotional expression across three distinct groups: 
    <span style="font-weight: bold;">hearing</span> individuals, 
    <span style="font-weight: bold;">deaf</span> individuals, 
    and 
    <span style="font-weight: bold;">CODA</span> (Children of Deaf Adults) individuals. 
    Hearing individuals rely primarily on speech, 
    while deaf individuals primarily use sign language for communication. 
    CODA individuals, on the other hand, have been exposed to both speech and signing from early childhood due to their deaf parents, 
    resulting in unique language and communication experiences.
  </p>


  <p3>Data</p3>
<div id="data-section">
  <ul>
    <li>A <span style="text-decoration: underline;">controlled experiment</span> was conducted to acquire data.</li>
    <li>The data consists of short videos of hearing people talking, deaf people signing, and CODA people doing both.</li>
    <li>Each video has a duration of approximately <span style="text-decoration: underline;"> 3-6 seconds</span>.</li>
    <li>Emotion labeling was done for each video, with <span style="text-decoration: underline;"> 4 different emotions</span>: <span style="color: red; font-weight: bold;">Angry</span>, <span style="color: green; font-weight: bold;">Happy</span>, <span style="color: blue; font-weight: bold;">Sad</span>, and <span style="color: #37474f; font-weight: bold;">Neutral</span>.</li>
  </ul>
</div>

  <p3>Data Distribution</p3>
<p>
	<img src="webpage_files\data_distribution\deaf_plot.png" alt="data_dist_deaf">
	<img src="webpage_files\data_distribution\hearing_plot.png" alt="data_dist_hearing">
<br>
	<img src="webpage_files\data_distribution\coda_speech_plot.png" alt="data_dist_coda_speech">
	<img src="webpage_files\data_distribution\coda_sign_plot.png" alt="data_dist_coda_sign">
</p>  


<p3>MediaPipe Pose: Advanced Human Pose Estimation Using Deep Learning</p3>
<p>
MediaPipe Pose is an advanced framework developed by Google that employs deep learning techniques to accurately estimate human body poses. With 33 3D landmarks, it provides a high-fidelity representation of body movements.
</p>
<img src="webpage_files/mediapipe-33-keypoints.jpg" alt="mediapipe" style="width: 600px; height: auto;">
<p>
For our study, we focused on four specific points of interest:
<div id="data-section">
<ul>
    <li>The right wrist (Landmark 15)</li>
    <li>The left wrist (Landmark 16)</li>
    <li>The midpoint between the eyes (average of Landmarks 2 and 5)</li>
    <li>The midpoint between the shoulders (average of Landmarks 11 and 12)</li>
</ul>
</div>

</p>

<p3>Visualizing Landmarks during Emotional Expression</p3>

<p>
To gain insights into emotional expression during signing or speaking the angry sentence "You're wasting my time!" in Hebrew, we present three GIFs for each category. The first GIF shows the complete set of captured landmarks from MediaPipe Pose, along with the underlying skeleton. The second GIF focuses solely on our four points of interest, offering a closer examination. The third GIF presents the four landmarks in all frames of the video, providing a comprehensive view of their movements and patterns throughout the entire sequence.
<br>
<br>
<span style="font-weight: bold;">Deaf:</span> <br>
<img src="webpage_files/mediapipe_examples/deaf/frame_by_frame.gif" alt="deaf">
<img src="webpage_files/mediapipe_examples/deaf/our_points.gif" alt="deaf">
<img src="webpage_files/mediapipe_examples/deaf/all_frames.gif" alt="deaf">
<br>
<span style="font-weight: bold;">Hearing:</span> <br> 
<img src="webpage_files/mediapipe_examples/hearing/frame_by_frame.gif" alt="hearing">
<img src="webpage_files/mediapipe_examples/hearing/our_points.gif" alt="hearing">
<img src="webpage_files/mediapipe_examples/hearing/all_frames.gif" alt="hearing">
<br>
<span style="font-weight: bold;">CODA sign:</span> <br>
<img src="webpage_files/mediapipe_examples/CODA_sign/frame_by_frame.gif" alt="coda-sign">
<img src="webpage_files/mediapipe_examples/CODA_sign/our_points.gif" alt="coda-sign">
<img src="webpage_files/mediapipe_examples/CODA_sign/all_frames.gif" alt="coda-sign">
<br>
<span style="font-weight: bold;">CODA speech:</span> <br> 
<img src="webpage_files/mediapipe_examples/CODA_speech/frame_by_frame.gif" alt="coda-speech">
<img src="webpage_files/mediapipe_examples/CODA_speech/our_points.gif" alt="coda-speech">
<img src="webpage_files/mediapipe_examples/CODA_speech/all_frames.gif" alt="coda-speech">
</p>

<p3>Video Processing: A Two-Stage Approach</p3>

<p>
We introduce a two-stage approach for video processing, designed to extract meaningful features from video frames. The first stage involves processing frame-wise information, while the second stage focuses on aggregating frame information to derive video-wise features.
</p>
<p2>Frame-wise Processing:</p2>
<p>
We start by identifying four key landmarks in each frame: the left wrist, right wrist, mid eyes, and mid shoulders. From these landmarks, we extract a set of <span style="font-weight: bold;">18</span> features per frame. These features include the x, y, and z coordinates for each landmark, as well as the Euclidean speed. The Euclidean speed is calculated by measuring the distance between the current frame's coordinates and the coordinates of the previous frame. For the initial frames without a previous frame, the Euclidean speed is set to zero. Additionally, we measure the distance of the wrists from the body plane, which is defined using the mid shoulders, left hip, and right hip landmarks (landmarks 23 and 24 in MediaPipe Pose). <br><br>
<em> (18 FEATURES: for all landmarks - x, y, z coordinates and Euclidean speed ; only for the two wrists - distance from body plane.) </em>
</p>

<p2>Aggregation of Frame Information:</p2>
<p>
In the second stage, we aggregate the frame information to derive video-wise features. This process yields a total of <span style="font-weight: bold;">36</span> features. These features include the variance* of the x, y, and z coordinates across all frames, as well as the largest eigenvalue of the covariance matrix. The covariance matrix represents the relationship between variables (x, y, z) across frames. Computing the largest eigenvalue helps identify the principal component, which indicates the direction of maximum variance. Additionally, we calculate the mean and variance of the speed, the total distance covered by each landmark (sum of all Euclidean distances), and the volume of the 3D movement for each landmark (volume of the convex hull formed by the landmark points). For the wrists, we also compute the mean and variance of the distance from the body plane. <br>
To determine the distinction between the 'Dominant' and 'Non-dominant' wrists, we identified the wrist with the larger volume as the 'Dominant' wrist. Thus, we replaced the labels 'Left' and 'Right' with 'Dominant' and 'Non-dominant' accordingly.<br>
Eventually, we performed data normalization to achieve a standard deviation of 1 and a mean of zero.

<br><br>
* We use the variance instead of the mean because using the mean could introduce bias in the machine learning process. Subjects from different labels might have different mean values simply because of their positions relative to the camera. By using variance, we focus on the spread or dispersion of the data points around the reference point, which allows us to capture the relative differences more accurately and avoid potential biases introduced by positional variations.

<br><br>
<em> (36 FEATURES: for all landmarks - variance of x, y, z coordiantes, largest eigenvalue, mean and variance of speed, volume of the convex hull, total distance covered ; only for the two wrists - mean and variance of distance from body) </em>

<br><br>
<img src="webpage_files/dom_hand/by_category.png" alt="dom1">
<img src="webpage_files/dom_hand/by_emotion.png" alt="dom2">

</p>


<p2>Normalization to Baseline and Scaling:</p2>
<p>
In our process of feature normalization, the goal is to bring the features to a common reference point, known as the baseline. This baseline is represented by the "Neutral" label. 
<br>
<strong>Normalization.</strong> For each subject, we calculate the average of all sentences' features corresponding to instances labeled as "Neutral." This average, obtained by considering the neutral instances as a whole, represents the characteristic features of the baseline. Next, we perform normalization on each sample of that subject by subtracting the computed average. This process aligns the subject's features with the neutral baseline, ensuring that variations are relative to the neutral reference point.
<br>
<strong>Scaling.</strong> After normalization, we proceed to scale each subject's sample by dividing it by the baseline average. This step allows standardized comparisons across subjects and makes the normalization process relative to the baseline. It represents the normalized features as proportions of their deviation from the average of the "Neutral" instances.
</p>

<p3>Emotion Classifier</p3>
<p>
We utilize a <em>random forest</em> classifier comprised of 100 trees. In order to emulate a balanced dataset, we adjust the weights assigned to each class in inverse proportion to their frequencies in the input data. For each execution, we select a different subset of our categories, including hearing, deaf, CODA sign, and CODA speech, as our data. To assess the performance of our model, we employ an 80-20 random split for training and testing, and utilize a 5-fold technique to evaluate the results on the hold-out test set. This technique involves averaging accuracy over the 5 folds and summing the confusion matrix. Our classifier takes in 36 numeric features extracted from the video processing stage as input, with the target variable being the emotion (anger, sadness, happiness, or neutrality).
</p>
<img src="webpage_files\RandomForestClassifier_Image.png" alt="random_forest" style="width: 500px; height: auto;">
<br><br>
<p2>Results</p2>
<p>
The confusion matrix is presented along with the accuracy metric and the four most significant features. 
</p>
<p2>A separate classifier for each category (hearing, deaf, CODA speech, CODA sign): <br><br></p2>
<img src="webpage_files\emotion_classifier\conf_hearing.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\conf_CODA speech.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\conf_deaf.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\conf_CODA sign.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<br>
<img src="webpage_files\emotion_classifier\feat_hearing.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\feat_CODA speech.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\feat_deaf.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\feat_CODA sign.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<br> <br>

<p2>A separate classifier for signers (hearing, CODA speech) and speakers (deaf, CODA sign):<br><br></p2>
<img src="webpage_files\emotion_classifier\conf_hearing_CODA speech.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\conf_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<br>
<img src="webpage_files\emotion_classifier\feat_hearing_CODA speech.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\emotion_classifier\feat_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<br> <br>

<p2>An all-category emotion classifier:<br><br></p2>
<img src="webpage_files\emotion_classifier\conf_hearing_CODA speech_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 300px; height: auto;">
<img src="webpage_files\emotion_classifier\feat_hearing_CODA speech_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 300px; height: auto;">
<br> <br>

<br>
<br>


<p3>Category Classifier</p3>
<p>
We utilize a <em>random forest</em> classifier comprised of 100 trees. In order to emulate a balanced dataset, we adjust the weights assigned to each class in inverse proportion to their frequencies in the input data. For each execution, we select a different subset of our emotions as our data. To assess the performance of our model, we employ an 80-20 random split for training and testing, and utilize a 5-fold technique to evaluate the results on the hold-out test set. This technique involves averaging accuracy over the 5 folds and summing the confusion matrix. Our classifier takes in 36 numeric features extracted from the video processing stage as input, with the target variable being the category (hearing, deaf, CODA sign, and CODA speech).
</p>
<img src="webpage_files\RandomForestClassifier_Image.png" alt="random_forest" style="width: 500px; height: auto;">
<br><br>
<p2>Results</p2> 
<br><br>
<p2>Hearing vs. Deaf vs. CODA speech vs. CODA sign:<br><br></p2>
<img src="webpage_files\category_classifier\conf_hearing_deaf_CODA speech_CODA sign_ALL.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf_CODA speech_CODA sign_ALL.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;"
<br><br><br>
<p2>Speakers vs. Signers (no CODA included):<br><br></p2>
<img src="webpage_files\category_classifier\conf_hearing_deaf.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;"
<br><br><br>
<p2>Speakers vs. Signers (no CODA included): Emotion-Wise Analysis<br><br></p2>
<img src="webpage_files\category_classifier\conf_hearing_deaf_angry.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\conf_hearing_deaf_happy.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\conf_hearing_deaf_sad.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\conf_hearing_deaf_neutral.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<br>
<img src="webpage_files\category_classifier\feat_hearing_deaf_angry.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf_happy.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf_sad.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf_neutral.png" alt="data_dist_coda_sign" style="width: 270px; height: auto;">

<br><br><br>
<p2>Speakers vs. Signers (CODA included):<br><br></p2>
<img src="webpage_files\category_classifier\conf_hearing_deaf_CODA speech_CODA sign.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_deaf_CODA speech_CODA sign.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<br><br><br>
<p2>Speakers vs. CODA Speakers:<br><br></p2>
<img src="webpage_files\category_classifier\conf_hearing_CODA speech.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<img src="webpage_files\category_classifier\feat_hearing_CODA speech.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;"
<br><br><br>
<p2>Deaf Signers vs. CODA Signers:<br><br></p2>
<img src="webpage_files\category_classifier\conf_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;">
<img src="webpage_files\category_classifier\feat_deaf_CODA sign.png" alt="data_dist_coda_sign" style="width: 350px; height: auto;"
<br><br><br>
<br><br><br>

<p3>What are the distinguishing features of each emotion?</p3>
<p>
<p2>Hearing: <br> </p2>
<img src="webpage_files\binary_classifier\conf_hearing_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_hearing_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_hearing_neutral-happy.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_hearing_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_hearing_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_hearing_neutral-sad.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_hearing_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_hearing_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_hearing_neutral-angry.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">

<br> <br> <br>

<p2>CODA Speech: <br> </p2>
<img src="webpage_files\binary_classifier\conf_CODA speech_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA speech_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA speech_neutral-happy.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_CODA speech_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA speech_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA speech_neutral-sad.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_CODA speech_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA speech_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA speech_neutral-angry.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">

<br> <br> <br>

<p2>Deaf: <br> </p2>
<img src="webpage_files\binary_classifier\conf_deaf_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_deaf_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_deaf_neutral-happy.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_deaf_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_deaf_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_deaf_neutral-sad.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_deaf_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_deaf_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_deaf_neutral-angry.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">

<br> <br> <br>


<p2>CODA Speech: <br> </p2>
<img src="webpage_files\binary_classifier\conf_CODA sign_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA sign_neutral-happy.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA sign_neutral-happy.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_CODA sign_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA sign_neutral-sad.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA sign_neutral-sad.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">
<br> 
<img src="webpage_files\binary_classifier\conf_CODA sign_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\feat_CODA sign_neutral-angry.png" alt="data_dist_coda_sign" style="width: 250px; height: auto;">
<img src="webpage_files\binary_classifier\bestfeat_CODA sign_neutral-angry.png" alt="data_dist_coda_sign" style="width: 650px; height: 250px;">

<br> <br> <br>

</p>

</body>
</html>
